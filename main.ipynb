{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gosoft Data Scientist Assesment\n",
    "## Demand Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Tanat Metmaolee\n",
    "\n",
    "__Goal__: To predict 3 months of item-level sales data at different store locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the Objective\n",
    "1. The Dataset is considered a [Time Series Analysis](https://www.geeksforgeeks.org/time-series-analysis-and-forecasting/)\n",
    "2. To create/choose an optimal statistics/machine learning model to predict item-level sales in advance.\n",
    "3. Evaluate and Propose how to deal with future demanding forecast to improve profit margins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trend\n",
    "2. Seasonality\n",
    "3. Cyclic\n",
    "4. Irregular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiled by Python 3.12.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bareck/anaconda3/envs/ds_projects/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Time Series Model\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import optuna\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgb\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# etc\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('demand-forecasting/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('demand-forecasting/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing/Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  store  item  sales\n",
       "0  2013-01-01      1     1     13\n",
       "1  2013-01-02      1     1     11\n",
       "2  2013-01-03      1     1     14\n",
       "3  2013-01-04      1     1     13\n",
       "4  2013-01-05      1     1     10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        date  store  item\n",
       "0   0  2018-01-01      1     1\n",
       "1   1  2018-01-02      1     1\n",
       "2   2  2018-01-03      1     1\n",
       "3   3  2018-01-04      1     1\n",
       "4   4  2018-01-05      1     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 913000 entries, 0 to 912999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   date    913000 non-null  object\n",
      " 1   store   913000 non-null  int64 \n",
      " 2   item    913000 non-null  int64 \n",
      " 3   sales   913000 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 27.9+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null/Na values check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date     0\n",
       "store    0\n",
       "item     0\n",
       "sales    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date     0\n",
       "store    0\n",
       "item     0\n",
       "sales    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['item'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df['store'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`store`, `item` Features are cleaned since they're discrete. \n",
    "\n",
    "`sales` Label could have some outliers to be checked.\n",
    "\n",
    "`date` Feature's format needs to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `date` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use in-built pandas Method -> `pd.to_datetime()` to add new columns as `day` `Month` `Year`\n",
    "\n",
    "for machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to keep the `date` column as well for future analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add more features for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create Time-based Features\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Datetime Features\n",
    "    df['day_of_week'] = df['date'].dt.day_of_week  # zero-indexed, start from Monday\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    \n",
    "    # month & day_of_week cyclical features\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Lag Features\n",
    "    for lag_amount in (1, 7, 30, 60, 365):\n",
    "        df[f'lag_{lag_amount}'] = df.groupby(['store', 'item'])['sales'].shift(lag_amount)\n",
    "        \n",
    "    # Rolling Mean Features\n",
    "    for window in [7, 14, 30, 60]:\n",
    "        df[f'rolling_mean_{window}'] = df.groupby(['store', 'item'])['sales'].transform(\n",
    "            lambda x: x.rolling(window=window).mean())\n",
    "        df[f'rolling_std_{window}'] = df.groupby(['store', 'item'])['sales'].transform(\n",
    "            lambda x: x.rolling(window=window).std())\n",
    "        df[f'rolling_max_{window}'] = df.groupby(['store', 'item'])['sales'].transform(\n",
    "            lambda x: x.rolling(window=window).max())\n",
    "        df[f'rolling_min_{window}'] = df.groupby(['store', 'item'])['sales'].transform(\n",
    "            lambda x: x.rolling(window=window).min())\n",
    "        \n",
    "    # Target Encoding Features\n",
    "    # Store average sales\n",
    "    store_means = df.groupby('store')['sales'].mean().to_dict()\n",
    "    df['store_mean_sales'] = df['store'].map(store_means)\n",
    "    \n",
    "    # Item average sales\n",
    "    item_means = df.groupby('item')['sales'].mean().to_dict()\n",
    "    df['item_mean_sales'] = df['item'].map(item_means)\n",
    "    \n",
    "    # Store-item average sales\n",
    "    store_item_means = df.groupby(['store', 'item'])['sales'].mean().to_dict()\n",
    "    df['store_item_mean_sales'] = df.apply(lambda x: store_item_means.get((x['store'], x['item']), 0), axis=1)\n",
    "    \n",
    "    # Month-store and month-item interaction effects\n",
    "    month_store_means = df.groupby(['month', 'store'])['sales'].mean().to_dict()\n",
    "    df['month_store_mean_sales'] = df.apply(lambda x: month_store_means.get((x['month'], x['store']), 0), axis=1)\n",
    "    \n",
    "    month_item_means = df.groupby(['month', 'item'])['sales'].mean().to_dict()\n",
    "    df['month_item_mean_sales'] = df.apply(lambda x: month_item_means.get((x['month'], x['item']), 0), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cube Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-Built pandas Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-Level Drill Down Test\n",
    "df[(df['store'] == 1) & (df['item'] == 2)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(df, store_number=None, item_number=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if store_number is None and item_number is None:\n",
    "        return df\n",
    "    elif item_number is None:\n",
    "        return df[df['store'] == store_number]\n",
    "    else:\n",
    "        return df[(df['store'] == store_number) & (df['item'] == item_number)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection (Outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the distributions are considered normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store 1 item 1 sample\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.displot(get_df(df, 1, 1).sales, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might have to detect anomalies on item-level since there is not enough information provided by dataset\n",
    "\n",
    "Things that might need to be checked:\n",
    "- Z-Score\n",
    "- Interquartile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inter Quartile Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$IQR = Q_3 - Q_1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers Data Point:\n",
    "\n",
    "$outlier = (Q_1 - 1.5 \\times IQR)$ or $(Q_3 + 1.5 \\times IQR)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=get_df(df, 1, 1)['sales'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_outliers(df, column):\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    outliers = ((df[column] < (q1 - 1.5 * iqr)) | (df[column] > (q3 + 1.5 * iqr)))\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ z = \\frac{(x - \\mu)}{\\sigma} $\n",
    "\n",
    "- $z$ = Z-score\n",
    "- $x$ = the current (sales) value\n",
    "- $\\mu$ = mean\n",
    "- $\\sigma$ = SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_outliers(df, column, threshold=3):\n",
    "    return np.abs(zscore(df[column])) > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Z-Score with IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (store, item), group in df.groupby(['store', 'item']):\n",
    "    temp_df_iqr = iqr_outliers(group, 'sales')\n",
    "    temp_df_zscore = z_score_outliers(group, 'sales')\n",
    "    df.loc[group.index, 'outlier'] = temp_df_iqr | temp_df_zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['outlier'] == True]) / len(df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.49% is consider very low and acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `sales` Outliers with __median__ value of each item-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (store, item), group in df.groupby(['store', 'item']):\n",
    "    median_value = group[group['outlier'] == False]['sales'].median()\n",
    "    df.loc[group.index[group['outlier']], 'sales'] = int(median_value)  # Cast Median to int type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=get_df(df, 1, 1)['sales'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overall Data\n",
    "2. Correlation\n",
    "3. Seasonality\n",
    "4. Trend & Decomposition\n",
    "5. Noise\n",
    "6. Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_number = 1    # From 1 to 10\n",
    "item_number = 1     # From 1 to 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Sales Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(get_df(df, store_number, item_number), \n",
    "              x='date', \n",
    "              y='rolling_mean_7',\n",
    "              title=f'Store {store_number} Item {item_number} Average Sales')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = get_df(df, store_number, item_number).drop(['store', 'item', 'outlier'], axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(f'Feature Correlation Heatmap of store {store_number} item {item_number}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelation (ACF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plot_acf(get_df(df, store_number, item_number)['sales'], lags=70)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title(f'Autocorrelation of Store {store_number} item {item_number}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the items is most likely fall under the same lags which is 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Autocorrelation (PACF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plot_pacf(get_df(df, store_number, item_number)['sales'], lags=50)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title(f'Partial Autocorrelation of Store {store_number} item {item_number}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend & Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole dataset month Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df.set_index(\"date\").resample(\"ME\")[\"sales\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonality from Sample (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(get_df(df, 1, 1).iloc[:30].set_index('date')['sales'], title=f\"First month's sales store (1, 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(get_df(df, 1, 1).set_index(\"date\").resample(\"ME\")[\"sales\"].sum(), title='whole dataset sales from item (1, 1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dated_df = df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_multi_output = seasonal_decompose(get_df(dated_df, 1, 1)['sales'], model='multiplicative').trend\n",
    "trend_addict_output = seasonal_decompose(get_df(dated_df, 1, 1)['sales'], model='addictive').trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both trends are quite Horizontal or slightly Upward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.grid()\n",
    "plt.plot(trend_multi_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.grid()\n",
    "plt.plot(trend_addict_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df.set_index(\"date\").resample(\"YE\")[\"sales\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity (for stats model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADF Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null-hypothesis of ADF -> the time serie is non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_getter(df):\n",
    "    df = df.dropna()\n",
    "    \n",
    "    adf_result = adfuller(df, autolag='AIC')\n",
    "    print(f'ADF Statistic: {adf_result[0]}')\n",
    "    print(f'p-value: {adf_result[1]}')\n",
    "    print(f'n_lags: {adf_result[2]}')\n",
    "    \n",
    "    print('Critical Values:')\n",
    "    \n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f'{key}, {value}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(df):\n",
    "    df = df.dropna()\n",
    "    adf_result = adfuller(df)\n",
    "    p_value = adf_result[1]\n",
    "    \n",
    "    return p_value < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_p_value(df):\n",
    "    result = adfuller(df)\n",
    "    \n",
    "    return result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_stationary_items(df, column='sales'):\n",
    "    non_station_store_items = []\n",
    "        \n",
    "    for (store, item), group in df.groupby(['store', 'item']):\n",
    "        if not check_stationarity(group[column]):\n",
    "            non_station_store_items.append((store, item))\n",
    "    \n",
    "    return non_station_store_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_station_products = get_non_stationary_items(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(non_station_products))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46 items in the whole-sale (500 items) are non-stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff_1'] = df.groupby(['store', 'item'])['sales'].transform(lambda x: x.diff())\n",
    "df['diff_2'] = df.groupby(['store', 'item'])['diff_1'].transform(lambda x: x.diff())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first non-stationary item is at store: 1, item: 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_13 = get_df(df, 1, 13).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(df_1_13['diff_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plot_acf(get_df(df, 1, 13)['diff_1'].dropna(), lags=20)\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title(f'Autocorrelation of Store {1} item {13}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plot_pacf(get_df(df, 1, 13)['diff_1'].dropna(), lags=30)\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title(f'Partial Autocorrelation of Store {1} item {13}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box-Cox Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption from Box-Cox Transformation:\n",
    "- Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_cox_df, box_cox_lambda = stats.boxcox(get_df(df, 1, 13)['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time Series\n",
    "    1. ARIMA\n",
    "    2. SARIMAX\n",
    "    3. Prophet\n",
    "    5. LSTM\n",
    "- ML\n",
    "    1. NNR\n",
    "    2. Catboost\n",
    "    3. XGBoost\n",
    "    4. LightGBM\n",
    "    5. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues\n",
    "1. Model Selection: to find the quickest and the most accurate.\n",
    "2. Cross-Validation Method: `Timeseriessplit()` but is this the most optimal?\n",
    "3. Hyperparameter tuning: for SARIMA, XGBoost, LightGBM\n",
    "4. Dependent features: for future datapoints, the features (such as `lag`, `rolling_mean`) have to be created dynamically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['day_of_week', 'month', 'year', 'lag_1', 'lag_7', 'lag_30', 'rolling_mean_7', 'rolling_mean_30']\n",
    "target = ['sales']\n",
    "n_splits = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_items = df[['store', 'item']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MAE\n",
    "- MSE\n",
    "- RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(n_splits):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    return tscv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data has seasonality, SARIMA would be more suitable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarima_model(df, item, store, horizon=90):\n",
    "    subset = df[get_df(df, store, item)]\n",
    "    subset = subset.set_index(\"date\")[\"sales\"]\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=4)\n",
    "    \n",
    "    def objective(trial):\n",
    "        order = (trial.suggest_int(\"p\", 0, 3),  \n",
    "                 trial.suggest_int(\"d\", 0, 2),  \n",
    "                 trial.suggest_int(\"q\", 0, 3))  \n",
    "        seasonal_order = (trial.suggest_int(\"P\", 0, 3),  \n",
    "                          trial.suggest_int(\"D\", 0, 2),  \n",
    "                          trial.suggest_int(\"Q\", 0, 3), \n",
    "                          7)  # Weekly seasonality\n",
    "        errors = []\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(subset):\n",
    "            train, test = subset.iloc[train_idx], subset.iloc[test_idx]\n",
    "            model = SARIMAX(train, order=order, seasonal_order=seasonal_order)\n",
    "            model_fit = model.fit(disp=False)\n",
    "            forecast = model_fit.forecast(len(test))\n",
    "            errors.append(np.mean((forecast - test) ** 2))\n",
    "        \n",
    "        return np.mean(errors)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    final_model = SARIMAX(subset, \n",
    "                           order=(best_params[\"p\"], best_params[\"d\"], best_params[\"q\"]),\n",
    "                           seasonal_order=(best_params[\"P\"], best_params[\"D\"], best_params[\"Q\"], 7))\n",
    "    final_fit = final_model.fit(disp=False)\n",
    "   \n",
    "    forecast = final_fit.forecast(horizon)\n",
    "    \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I observed through the experiments, SARIMA requires each product's `p`, `d`, `q` variables (which we need to find them manually or run `auto_arima()` method) that would take too long to execute (or inaccurate with global training). \n",
    "\n",
    "Therefore, I decided to use machine learning model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(trial, model_type, train_X, train_y):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'n_estimators': 500,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    if model_type == \"lightgbm\":\n",
    "        params.update({\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 10, 200),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "        })\n",
    "        model_class = lgb.LGBMRegressor\n",
    "    elif model_type == \"xgboost\":\n",
    "        params.update({\n",
    "            'gamma': trial.suggest_loguniform('gamma', 1e-8, 10.0),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n",
    "        })\n",
    "        model_class = xgb.XGBRegressor\n",
    "\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_index, test_index in tscv.split(train_X):\n",
    "        X_train, X_test = train_X.iloc[train_index], train_X.iloc[test_index]\n",
    "        y_train, y_test = train_y.iloc[train_index], train_y.iloc[test_index]\n",
    "\n",
    "        model = model_class(**params)\n",
    "        \n",
    "        if model_type == \"lightgbm\":\n",
    "            model.fit(X_train, y_train, eval_set=[(X_test, y_test)], \n",
    "                      callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "        elif model_type == \"xgboost\":\n",
    "            early_stopping = EarlyStopping(rounds=50, metric_name=\"rmse\", data_name=\"validation\")\n",
    "            model.fit(\n",
    "                X_train, y_train, \n",
    "                eval_set=[(X_test, y_test)],\n",
    "                callbacks=[early_stopping]\n",
    "            )\n",
    "        \n",
    "        preds = model.predict(X_test)\n",
    "        rmse = np.sqrt(root_mean_squared_error(y_test, preds))\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost/LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(df, model_type, n_trials=10):\n",
    "    \"\"\" Train and save LightGBM/XGBoost models \"\"\"\n",
    "    store_items = df[['store', 'item']].drop_duplicates()\n",
    "    trained_models = {}\n",
    "\n",
    "    for store, item in tqdm(store_items.values, desc=f\"Training {model_type}\"):\n",
    "        df_subset = get_df(df, store, item).copy()\n",
    "\n",
    "        if len(df_subset) < 100:\n",
    "            continue  \n",
    "\n",
    "        train_X, train_y = df_subset[features], df_subset[target]\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: tune_model(trial, model_type, train_X, train_y), n_trials=n_trials)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        model_class = lgb.LGBMRegressor if model_type == \"lightgbm\" else xgb.XGBRegressor\n",
    "        final_model = model_class(**best_params)\n",
    "        final_model.fit(train_X, train_y)\n",
    "\n",
    "        if model_type == \"lightgbm\":\n",
    "            model_filename = f\"lightgbm_training_models/{model_type}_model_store{store}_item{item}.pkl\"\n",
    "            joblib.dump(final_model, model_filename)\n",
    "        else:\n",
    "            model_filename = f\"xgboost_training_models/{model_type}_model_store{store}_item{item}.pkl\"\n",
    "            joblib.dump(final_model, model_filename)\n",
    "        trained_models[(store, item)] = final_model\n",
    "\n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgbm = train_models(df, \"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = train_models(df, \"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lightgbm_model_store1_item1.pkl\", \"rb\") as f:\n",
    "    loaded_lgbm_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_results = []\n",
    "for model_type, models in [(\"lightgbm\", models_lgbm), (\"xgboost\", models_xgb)]:\n",
    "    for (store, item), model in tqdm(models.items(), desc=f\"Forecasting with {model_type}\"):\n",
    "        df_subset = get_df(df, store, item).copy()\n",
    "        future_dates = pd.date_range(start=df_subset['date'].max() + pd.Timedelta(days=1), periods=90, freq='D')\n",
    "\n",
    "        future_df = pd.DataFrame({'date': future_dates, 'store': store, 'item': item})\n",
    "        future_df['day_of_week'] = future_df['date'].dt.dayofweek\n",
    "        future_df['month'] = future_df['date'].dt.month\n",
    "        future_df['year'] = future_df['date'].dt.year\n",
    "        future_df['is_weekend'] = (future_df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "        latest_data = df_subset.tail(30)\n",
    "        for lag in [1, 7, 14, 30]:\n",
    "            future_df[f'lag_{lag}'] = latest_data['sales'].values[-lag] if len(latest_data) >= lag else 0\n",
    "\n",
    "        for ma in [7, 30]:\n",
    "            future_df[f'rolling_mean_{ma}'] = latest_data['sales'].rolling(window=ma).mean().values[-1] if len(latest_data) >= ma else 0\n",
    "\n",
    "        future_df['sales_predicted'] = model.predict(future_df[features])\n",
    "        future_df['model'] = model_type\n",
    "        forecast_results.append(future_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecasts(forecast_df):\n",
    "    \"\"\" Compare LightGBM vs XGBoost forecasts \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    sample_store, sample_item = forecast_df.iloc[0]['store'], forecast_df.iloc[0]['item']\n",
    "    df_sample = forecast_df[(forecast_df['store'] == sample_store) & (forecast_df['item'] == sample_item)]\n",
    "\n",
    "    sns.lineplot(data=df_sample, x=\"date\", y=\"sales_predicted\", hue=\"model\", palette={\"lightgbm\": \"blue\", \"xgboost\": \"red\"})\n",
    "\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Predicted Sales\")\n",
    "    plt.title(f\"Forecast Comparison for Store {sample_store}, Item {sample_item}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast(model_lgbm, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_results = []\n",
    "for model_type, models in [(\"lightgbm\", models_lgbm), (\"xgboost\", models_xgb)]:\n",
    "    for (store, item), model in tqdm(models.items(), desc=f\"Forecasting with {model_type}\"):\n",
    "        df_subset = df[(df['store'] == store) & (df['item'] == item)].copy()\n",
    "        future_dates = pd.date_range(start=df_subset['date'].max() + pd.Timedelta(days=1), periods=90, freq='D')\n",
    "\n",
    "        future_df = pd.DataFrame({'date': future_dates, 'store': store, 'item': item})\n",
    "        future_df['day_of_week'] = future_df['date'].dt.dayofweek\n",
    "        future_df['month'] = future_df['date'].dt.month\n",
    "        future_df['year'] = future_df['date'].dt.year\n",
    "        future_df['is_weekend'] = (future_df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "        latest_data = df_subset.tail(30)\n",
    "        for lag in [1, 7, 14, 30]:\n",
    "            future_df[f'lag_{lag}'] = latest_data['sales'].values[-lag] if len(latest_data) >= lag else 0\n",
    "\n",
    "        for ma in [7, 30]:\n",
    "            future_df[f'rolling_mean_{ma}'] = latest_data['sales'].rolling(window=ma).mean().values[-1] if len(latest_data) >= ma else 0\n",
    "\n",
    "        future_df['sales_predicted'] = model.predict(future_df[features])\n",
    "        future_df['model'] = model_type\n",
    "        forecast_results.append(future_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(lgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future prediction storage\n",
    "predictions = []\n",
    "\n",
    "# Iterate over each store-item combination\n",
    "for store in df['store'].unique():\n",
    "    for item in df['item'].unique():\n",
    "        # Filter historical data for the current store-item combination\n",
    "        historical_data = df[(df['store'] == store) & (df['item'] == item)].copy()\n",
    "\n",
    "        # Generate future dates\n",
    "        future_data = pd.DataFrame({'date': future_dates, 'store': store, 'item': item})\n",
    "        future_data['day_of_week'] = future_data['date'].dt.dayofweek\n",
    "        future_data['month'] = future_data['date'].dt.month\n",
    "\n",
    "        # Initialize placeholders for lag & rolling features\n",
    "        last_known_sales = historical_data['sales'].tolist()\n",
    "\n",
    "        future_sales = []\n",
    "        for i in range(90):\n",
    "            # Create a single-row DataFrame for prediction\n",
    "            temp_row = future_data.iloc[i].copy()\n",
    "\n",
    "            # Compute lag features from last known sales\n",
    "            temp_row['lag_1'] = last_known_sales[-1] if len(last_known_sales) >= 1 else np.nan\n",
    "            temp_row['lag_7'] = last_known_sales[-7] if len(last_known_sales) >= 7 else np.nan\n",
    "            temp_row['lag_30'] = last_known_sales[-30] if len(last_known_sales) >= 30 else np.nan\n",
    "\n",
    "            # Compute rolling mean features\n",
    "            temp_row['rolling_mean_7'] = np.mean(last_known_sales[-7:]) if len(last_known_sales) >= 7 else np.nan\n",
    "            temp_row['rolling_mean_30'] = np.mean(last_known_sales[-30:]) if len(last_known_sales) >= 30 else np.nan\n",
    "\n",
    "            # Ensure all features exist\n",
    "            X_future = temp_row[features].to_frame().T  # Convert Series to DataFrame\n",
    "\n",
    "            X_future['store'] = X_future['store'].astype(int)  # If store numbers are numeric\n",
    "            X_future['item'] = X_future['item'].astype(int)\n",
    "            \n",
    "            lag_features = ['lag_7', 'lag_30', 'rolling_mean_7', 'rolling_mean_30']\n",
    "            X_future[lag_features] = X_future[lag_features].apply(pd.to_numeric, errors='coerce')\n",
    "            \n",
    "            # Predict sales for this day\n",
    "            predicted_sales = lgb_model.predict(X_future)[0]\n",
    "            future_sales.append(predicted_sales)\n",
    "\n",
    "            # Update known sales list for future iterations\n",
    "            last_known_sales.append(predicted_sales)\n",
    "\n",
    "        # Store predictions\n",
    "        predictions.extend(future_sales)\n",
    "\n",
    "# Store predictions as DataFrame\n",
    "predictions_df = pd.DataFrame({'date': np.tile(future_dates, len(df['store'].unique()) * len(df['item'].unique())),\n",
    "                               'store': np.repeat(df['store'].unique(), 90 * len(df['item'].unique())),\n",
    "                               'item': np.tile(np.repeat(df['item'].unique(), 90), len(df['store'].unique())),\n",
    "                               'predicted_sales': predictions})\n",
    "\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
