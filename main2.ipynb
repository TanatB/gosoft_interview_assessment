{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9519aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bareck/anaconda3/envs/ds_projects/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fc068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0771e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8722efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df):\n",
    "    \"\"\"Create time-based features from datetime column\"\"\"\n",
    "    print(\"Creating time-based features...\")\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Extract datetime components\n",
    "    df_features['year'] = df_features['date'].dt.year\n",
    "    df_features['month'] = df_features['date'].dt.month\n",
    "    df_features['day'] = df_features['date'].dt.day\n",
    "    df_features['dayofweek'] = df_features['date'].dt.dayofweek\n",
    "    df_features['quarter'] = df_features['date'].dt.quarter\n",
    "    df_features['dayofyear'] = df_features['date'].dt.dayofyear\n",
    "    df_features['is_weekend'] = df_features['dayofweek'].isin([5, 6]).astype(int)\n",
    "    df_features['is_month_start'] = df_features['date'].dt.is_month_start.astype(int)\n",
    "    df_features['is_month_end'] = df_features['date'].dt.is_month_end.astype(int)\n",
    "    df_features['week'] = df_features['date'].dt.isocalendar().week\n",
    "    \n",
    "    # Monthly cyclical encoding using sine and cosine\n",
    "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
    "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
    "    \n",
    "    # Weekly cyclical encoding\n",
    "    df_features['day_sin'] = np.sin(2 * np.pi * df_features['dayofweek'] / 7)\n",
    "    df_features['day_cos'] = np.cos(2 * np.pi * df_features['dayofweek'] / 7)\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6003cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, lag_days=[1, 7, 14, 28, 365]):\n",
    "    \"\"\"Create lag features for each store-item combination\"\"\"\n",
    "    print(\"Creating lag features...\")\n",
    "    df_lag = df.copy()\n",
    "    \n",
    "    # Sort by date for proper lagging\n",
    "    df_lag = df_lag.sort_values(['store', 'item', 'date'])\n",
    "    \n",
    "    # Create lag features\n",
    "    for lag in lag_days:\n",
    "        df_lag[f'sales_lag_{lag}'] = df_lag.groupby(['store', 'item'])['sales'].shift(lag)\n",
    "    \n",
    "    return df_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a67430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df, windows=[7, 14, 30, 90]):\n",
    "    \"\"\"Create rolling window statistics for each store-item combination\"\"\"\n",
    "    print(\"Creating rolling window features...\")\n",
    "    df_rolling = df.copy()\n",
    "    \n",
    "    # Sort by date for proper rolling calculations\n",
    "    df_rolling = df_rolling.sort_values(['store', 'item', 'date'])\n",
    "    \n",
    "    # Create rolling features\n",
    "    for window in windows:\n",
    "        df_rolling[f'sales_rolling_mean_{window}'] = df_rolling.groupby(['store', 'item'])['sales'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        df_rolling[f'sales_rolling_std_{window}'] = df_rolling.groupby(['store', 'item'])['sales'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).std())\n",
    "        df_rolling[f'sales_rolling_min_{window}'] = df_rolling.groupby(['store', 'item'])['sales'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).min())\n",
    "        df_rolling[f'sales_rolling_max_{window}'] = df_rolling.groupby(['store', 'item'])['sales'].transform(\n",
    "            lambda x: x.rolling(window, min_periods=1).max())\n",
    "    \n",
    "    return df_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dae3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_encoding(df):\n",
    "    \"\"\"Create target encodings for categorical variables\"\"\"\n",
    "    print(\"Creating target encodings...\")\n",
    "    df_target = df.copy()\n",
    "    \n",
    "    # Target encoding for store\n",
    "    store_means = df_target.groupby('store')['sales'].mean().to_dict()\n",
    "    df_target['store_mean_sales'] = df_target['store'].map(store_means)\n",
    "    \n",
    "    # Target encoding for item\n",
    "    item_means = df_target.groupby('item')['sales'].mean().to_dict()\n",
    "    df_target['item_mean_sales'] = df_target['item'].map(item_means)\n",
    "    \n",
    "    # Target encoding for store-item interaction\n",
    "    store_item_means = df_target.groupby(['store', 'item'])['sales'].mean().to_dict()\n",
    "    df_target['store_item_mean_sales'] = df_target.apply(lambda x: store_item_means.get((x['store'], x['item']), 0), axis=1)\n",
    "    \n",
    "    # Target encoding by time components\n",
    "    month_means = df_target.groupby('month')['sales'].mean().to_dict()\n",
    "    df_target['month_mean_sales'] = df_target['month'].map(month_means)\n",
    "    \n",
    "    dayofweek_means = df_target.groupby('dayofweek')['sales'].mean().to_dict()\n",
    "    df_target['dayofweek_mean_sales'] = df_target['dayofweek'].map(dayofweek_means)\n",
    "    \n",
    "    # Month-store interaction\n",
    "    month_store_means = df_target.groupby(['month', 'store'])['sales'].mean().to_dict()\n",
    "    df_target['month_store_mean_sales'] = df_target.apply(lambda x: month_store_means.get((x['month'], x['store']), 0), axis=1)\n",
    "    \n",
    "    # Month-item interaction\n",
    "    month_item_means = df_target.groupby(['month', 'item'])['sales'].mean().to_dict()\n",
    "    df_target['month_item_mean_sales'] = df_target.apply(lambda x: month_item_means.get((x['month'], x['item']), 0), axis=1)\n",
    "    \n",
    "    return df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8abaa81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"Combine all feature engineering steps\"\"\"\n",
    "    print(\"Preparing all features...\")\n",
    "    df_features = create_time_features(df)\n",
    "    df_features = create_lag_features(df_features)\n",
    "    df_features = create_rolling_features(df_features)\n",
    "    df_features = create_target_encoding(df_features)\n",
    "    \n",
    "    # Drop rows with NaN values (from lag/rolling features)\n",
    "    df_features_clean = df_features.dropna()\n",
    "    print(f\"Final features prepared. Shape after dropping NaN: {df_features_clean.shape}\")\n",
    "    \n",
    "    return df_features_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cddc5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and tuning functions\n",
    "def objective(trial, X_train, y_train, X_val, y_val, model_type=\"lightgbm\"):\n",
    "    \"\"\"Optuna objective function for hyperparameter tuning\"\"\"\n",
    "    if model_type == \"lightgbm\":\n",
    "        # LightGBM hyperparameters\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        \n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    elif model_type == \"xgboost\":\n",
    "        # XGBoost hyperparameters\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "            'verbosity': 0,\n",
    "            'seed': 42\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d3c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune_model(df_features, target_col='sales', model_type=\"lightgbm\", n_trials=50):\n",
    "    \"\"\"Train and tune model with time series cross-validation\"\"\"\n",
    "    print(f\"Training and tuning {model_type} model...\")\n",
    "    \n",
    "    # Prepare data for modeling\n",
    "    features = [col for col in df_features.columns if col not in ['date', target_col]]\n",
    "    X = df_features[features]\n",
    "    y = df_features[target_col]\n",
    "    \n",
    "    # Time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    best_params = None\n",
    "    best_models = []\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Further split test set into validation and test\n",
    "        split_point = len(X_test) // 2\n",
    "        X_val, X_test = X_test.iloc[:split_point], X_test.iloc[split_point:]\n",
    "        y_val, y_test = y_test.iloc[:split_point], y_test.iloc[split_point:]\n",
    "        \n",
    "        print(f\"Split sizes - Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
    "        \n",
    "        # Hyperparameter tuning with Optuna\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val, model_type), \n",
    "                      n_trials=n_trials)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Train model with best parameters\n",
    "        if model_type == \"lightgbm\":\n",
    "            model = lgb.LGBMRegressor(objective='regression', **best_params)\n",
    "        else:  # xgboost\n",
    "            model = xgb.XGBRegressor(objective='reg:squarederror', **best_params)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Test RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "        \n",
    "        # Save the model\n",
    "        best_models.append(model)\n",
    "    \n",
    "    # Return the model from the last fold and best parameters\n",
    "    return best_models[-1], best_params, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c911051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_future_dates(last_date, months=3):\n",
    "    \"\"\"Generate future dates for prediction\"\"\"\n",
    "    print(f\"Generating future dates for {months} months from {last_date}...\")\n",
    "    \n",
    "    future_dates = []\n",
    "    current_date = last_date + timedelta(days=1)\n",
    "    \n",
    "    # Calculate end date (approximately 3 months)\n",
    "    end_date = last_date + pd.DateOffset(months=months)\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        future_dates.append(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    return future_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b946826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_future_features(df, last_date, months=3, store_ids=None, item_ids=None):\n",
    "    \"\"\"Prepare features for future prediction\"\"\"\n",
    "    print(\"Preparing features for future prediction...\")\n",
    "    \n",
    "    # If store_ids and item_ids are not provided, use all from the dataframe\n",
    "    if store_ids is None:\n",
    "        store_ids = df['store'].unique()\n",
    "    if item_ids is None:\n",
    "        item_ids = df['item'].unique()\n",
    "    \n",
    "    # Generate future dates\n",
    "    future_dates = generate_future_dates(last_date, months)\n",
    "    \n",
    "    # Create combinations of store, item, and future dates\n",
    "    future_rows = []\n",
    "    for date in future_dates:\n",
    "        for store in store_ids:\n",
    "            for item in item_ids:\n",
    "                future_rows.append({\n",
    "                    'date': date,\n",
    "                    'store': store,\n",
    "                    'item': item\n",
    "                })\n",
    "    \n",
    "    # Create future dataframe\n",
    "    future_df = pd.DataFrame(future_rows)\n",
    "    \n",
    "    # Add time features\n",
    "    future_df = create_time_features(future_df)\n",
    "    \n",
    "    # For each store-item combination, we need historical data to create lag features\n",
    "    print(\"Creating historical context for future predictions...\")\n",
    "    \n",
    "    # Combine historical data with future dataframe\n",
    "    combined_df = pd.concat([\n",
    "        df[['date', 'store', 'item', 'sales']],\n",
    "        future_df[['date', 'store', 'item']].assign(sales=np.nan)\n",
    "    ]).sort_values(['store', 'item', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    # Create lag features\n",
    "    combined_df = create_lag_features(combined_df)\n",
    "    \n",
    "    # Create rolling features\n",
    "    combined_df = create_rolling_features(combined_df)\n",
    "    \n",
    "    # Create target encodings\n",
    "    combined_df = create_target_encoding(combined_df)\n",
    "    \n",
    "    # Extract only the future rows with features\n",
    "    future_with_features = combined_df[combined_df['date'] > last_date].copy()\n",
    "    \n",
    "    print(f\"Future features prepared. Shape: {future_with_features.shape}\")\n",
    "    return future_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5c3d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, df_features, future_df, features):\n",
    "    \"\"\"Generate predictions for future dates\"\"\"\n",
    "    print(\"Generating predictions for future dates...\")\n",
    "    \n",
    "    # Prepare future features for prediction\n",
    "    X_future = future_df[features]\n",
    "    \n",
    "    # Handle missing values in future features\n",
    "    for col in X_future.columns:\n",
    "        if X_future[col].isna().any():\n",
    "            # If a feature has missing values, fill with the mean from training data\n",
    "            mean_value = df_features[col].mean()\n",
    "            X_future[col] = X_future[col].fillna(mean_value)\n",
    "    \n",
    "    # Generate predictions\n",
    "    future_predictions = model.predict(X_future)\n",
    "    \n",
    "    # Add predictions to future dataframe\n",
    "    future_df['predicted_sales'] = future_predictions\n",
    "    \n",
    "    # Ensure no negative sales predictions\n",
    "    future_df['predicted_sales'] = future_df['predicted_sales'].clip(0)\n",
    "    \n",
    "    return future_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d5006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(df, future_df, store_id=1, item_id=1):\n",
    "    \"\"\"Visualize historical sales and future predictions for a specific store and item\"\"\"\n",
    "    print(f\"Visualizing predictions for Store {store_id}, Item {item_id}...\")\n",
    "    \n",
    "    # Filter historical data for the specific store and item\n",
    "    historical = df[(df['store'] == store_id) & (df['item'] == item_id)].copy()\n",
    "    historical = historical.sort_values('date')\n",
    "    \n",
    "    # Filter future predictions for the specific store and item\n",
    "    future = future_df[(future_df['store'] == store_id) & (future_df['item'] == item_id)].copy()\n",
    "    future = future.sort_values('date')\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(historical['date'], historical['sales'], label='Historical Sales', color='blue')\n",
    "    plt.plot(future['date'], future['predicted_sales'], label='Predicted Sales', color='red', linestyle='--')\n",
    "    \n",
    "    # Add vertical line separating historical and future data\n",
    "    last_historical_date = historical['date'].max()\n",
    "    plt.axvline(x=last_historical_date, color='green', linestyle='-', alpha=0.7,\n",
    "               label=f'Last Historical Date: {last_historical_date.strftime(\"%Y-%m-%d\")}')\n",
    "    \n",
    "    plt.title(f'Sales Prediction for Store {store_id}, Item {item_id}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'store_{store_id}_item_{item_id}_prediction.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb7a6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature_importance(model, features, model_type=\"lightgbm\"):\n",
    "    \"\"\"Evaluate and visualize feature importance\"\"\"\n",
    "    print(\"Analyzing feature importance...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    if model_type == \"lightgbm\":\n",
    "        importance = model.feature_importances_\n",
    "    else:\n",
    "        pass# xgbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61904d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Set paths\n",
    "    input_path = 'train.csv'\n",
    "    output_path = 'sales_predictions.csv'\n",
    "    \n",
    "    # 1. Load data\n",
    "    df = load_data(input_path)\n",
    "    \n",
    "    # Display some basic statistics\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # 2. Prepare features\n",
    "    df_features = prepare_features(df)\n",
    "    \n",
    "    # 3. Split dataset for training\n",
    "    # Use data up to last 3 months for training\n",
    "    last_date = df['date'].max()\n",
    "    train_end_date = last_date - pd.DateOffset(months=3)\n",
    "    \n",
    "    print(f\"\\nTraining on data up to: {train_end_date}\")\n",
    "    df_train = df_features[df_features['date'] <= train_end_date].copy()\n",
    "    df_val = df_features[df_features['date'] > train_end_date].copy()\n",
    "    \n",
    "    print(f\"Training data shape: {df_train.shape}\")\n",
    "    print(f\"Validation data shape: {df_val.shape}\")\n",
    "    \n",
    "    # 4. Train and tune model (choose model_type: \"lightgbm\" or \"xgboost\")\n",
    "    model_type = \"lightgbm\"  # Change to \"xgboost\" if preferred\n",
    "    n_trials = 50  # Number of hyperparameter optimization trials\n",
    "    \n",
    "    model, best_params, features = train_and_tune_model(\n",
    "        df_train, target_col='sales', model_type=model_type, n_trials=n_trials\n",
    "    )\n",
    "    \n",
    "    # 5. Evaluate on validation data\n",
    "    X_val = df_val[features]\n",
    "    y_val = df_val['sales']\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    val_mae = mean_absolute_error(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nValidation metrics - RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # 6. Prepare features for future prediction\n",
    "    future_df = prepare_future_features(df, last_date, months=3)\n",
    "    \n",
    "    # 7. Generate predictions for future dates\n",
    "    future_predictions = predict_future(model, df_features, future_df, features)\n",
    "    \n",
    "    # 8. Save predictions\n",
    "    future_predictions[['date', 'store', 'item', 'predicted_sales']].to_csv(\n",
    "        output_path, index=False\n",
    "    )\n",
    "    print(f\"\\nFuture predictions saved to {output_path}\")\n",
    "    \n",
    "    # 9. Visualize predictions for a few store-item combinations\n",
    "    for store_id in [1, 2, 3]:\n",
    "        for item_id in [1, 15, 25]:\n",
    "            visualize_predictions(df, future_predictions, store_id, item_id)\n",
    "    \n",
    "    # 10. Analyze feature importance\n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    evaluate_feature_importance(model, features, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b54d8e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3597455689.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    main()ost\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "main()ost\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', **best_params)\n",
    "        \n",
    "model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "print(f\"Test RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "        \n",
    "# Save the model\n",
    "best_models.append(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
